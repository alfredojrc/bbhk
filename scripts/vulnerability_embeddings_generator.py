#!/usr/bin/env python3
"""
Vector Embeddings Generator for Vulnerability Research
====================================================

Creates comprehensive semantic embeddings for vulnerability research data
to populate Qdrant with sophisticated search capabilities.

Author: AI Vector Embeddings Specialist
Date: 2025-08-26
Purpose: Enable semantic similarity search across vulnerability corpus
"""

import sqlite3
import json
import requests
import uuid
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import logging
from pathlib import Path
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VulnerabilityEmbeddingsGenerator:
    """
    Advanced embeddings generator for vulnerability research data.
    Creates semantic vectors for enhanced searchability and discovery.
    """
    
    def __init__(self, db_path: str = '/home/kali/bbhk/.swarm/memory.db', 
                 qdrant_url: str = 'http://localhost:6333'):
        self.db_path = db_path
        self.qdrant_url = qdrant_url
        self.model = None
        self.collections_config = self._get_collections_config()
        
    def _get_collections_config(self) -> Dict[str, Dict]:
        """Define Qdrant collection configurations for different data types."""
        return {
            "vulnerability_research": {
                "description": "Core vulnerability patterns and attack vectors",
                "vector_size": 384,  # all-MiniLM-L6-v2 dimension
                "distance": "Cosine",
                "on_disk_payload": True,
                "payload_fields": {
                    "vulnerability_type": {"type": "keyword", "index": True},
                    "severity": {"type": "keyword", "index": True},
                    "payout_range": {"type": "integer", "range": True},
                    "program_handle": {"type": "keyword", "index": True},
                    "created_at": {"type": "datetime", "index": True},
                    "source": {"type": "keyword", "index": True}
                }
            },
            "attack_patterns": {
                "description": "Attack methodologies and exploitation techniques",
                "vector_size": 384,
                "distance": "Cosine",
                "on_disk_payload": True,
                "payload_fields": {
                    "difficulty": {"type": "integer", "range": True},
                    "category": {"type": "keyword", "index": True},
                    "platform": {"type": "keyword", "index": True},
                    "tools_required": {"type": "keyword", "index": True},
                    "impact_level": {"type": "keyword", "index": True}
                }
            },
            "evidence_packages": {
                "description": "PoC descriptions and evidence summaries",
                "vector_size": 384,
                "distance": "Cosine",
                "on_disk_payload": True,
                "payload_fields": {
                    "poc_type": {"type": "keyword", "index": True},
                    "language": {"type": "keyword", "index": True},
                    "complexity": {"type": "integer", "range": True},
                    "verified": {"type": "bool", "index": True},
                    "success_rate": {"type": "float", "range": True}
                }
            },
            "research_methodology": {
                "description": "T.K.V.F., D.I.E. framework and research processes",
                "vector_size": 384,
                "distance": "Cosine",
                "on_disk_payload": True,
                "payload_fields": {
                    "framework": {"type": "keyword", "index": True},
                    "phase": {"type": "keyword", "index": True},
                    "effectiveness": {"type": "float", "range": True},
                    "lessons_learned": {"type": "text", "index": True}
                }
            },
            "target_platforms": {
                "description": "Technology-specific vulnerability patterns",
                "vector_size": 384,
                "distance": "Cosine", 
                "on_disk_payload": True,
                "payload_fields": {
                    "technology": {"type": "keyword", "index": True},
                    "version": {"type": "keyword", "index": True},
                    "deployment_status": {"type": "keyword", "index": True},
                    "audit_history": {"type": "text", "index": True},
                    "risk_level": {"type": "integer", "range": True}
                }
            }
        }
    
    def initialize_model(self) -> None:
        """Initialize the sentence transformer model for embeddings."""
        try:
            logger.info("Loading sentence transformer model...")
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            raise
    
    def create_collections(self) -> bool:
        """Create all required Qdrant collections with proper configuration."""
        logger.info("Creating Qdrant collections...")
        
        for collection_name, config in self.collections_config.items():
            try:
                # Delete existing collection if it exists
                requests.delete(f"{self.qdrant_url}/collections/{collection_name}")
                
                # Create collection with vector configuration
                collection_config = {
                    "vectors": {
                        "size": config["vector_size"],
                        "distance": config["distance"]
                    },
                    "on_disk_payload": config["on_disk_payload"]
                }
                
                response = requests.put(
                    f"{self.qdrant_url}/collections/{collection_name}",
                    json=collection_config
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ Created collection: {collection_name}")
                else:
                    logger.error(f"‚ùå Failed to create collection {collection_name}: {response.text}")
                    return False
                    
            except Exception as e:
                logger.error(f"Error creating collection {collection_name}: {e}")
                return False
        
        return True
    
    def generate_vulnerability_embeddings(self) -> List[Dict[str, Any]]:
        """Generate embeddings for vulnerability patterns from SQLite database."""
        logger.info("Generating vulnerability embeddings...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            
            # Get all vulnerability data with program context
            query = """
            SELECT v.*, p.name as program_name, p.url as program_url 
            FROM vulnerabilities v 
            LEFT JOIN programs p ON v.program_handle = p.handle
            """
            
            cursor = conn.execute(query)
            vulnerabilities = cursor.fetchall()
            
            embeddings_data = []
            
            for vuln in vulnerabilities:
                # Create comprehensive text for embedding
                vulnerability_text = self._create_vulnerability_text(vuln)
                
                # Generate embedding
                embedding = self.model.encode([vulnerability_text])[0].tolist()
                
                # Create payload with metadata
                payload = {
                    "id": str(uuid.uuid4()),
                    "vulnerability_type": vuln["type"] or "Unknown",
                    "severity": vuln["severity"] or "Medium",
                    "payout_range": f"{vuln['payout_min'] or 0}-{vuln['payout_max'] or 0}",
                    "program_handle": vuln["program_handle"] or "Unknown",
                    "program_name": vuln["program_name"] or "Unknown",
                    "pattern": vuln["pattern"] or "",
                    "exploit_code": vuln["exploit_code"] or "",
                    "cvss_score": vuln["cvss_score"] or 0.0,
                    "cve": vuln["cve"] or "",
                    "created_at": vuln["created_at"] or datetime.now().isoformat(),
                    "source": "database",
                    "text_content": vulnerability_text,
                    "effort_ratio": vuln["effort_ratio"] or "medium"
                }
                
                embeddings_data.append({
                    "id": payload["id"],
                    "vector": embedding,
                    "payload": payload
                })
            
            conn.close()
            logger.info(f"Generated {len(embeddings_data)} vulnerability embeddings")
            return embeddings_data
            
        except Exception as e:
            logger.error(f"Error generating vulnerability embeddings: {e}")
            return []
    
    def generate_attack_pattern_embeddings(self) -> List[Dict[str, Any]]:
        """Generate embeddings for attack patterns from documentation."""
        logger.info("Generating attack pattern embeddings...")
        
        attack_patterns = [
            {
                "name": "Business Logic Race Conditions",
                "description": "Exploiting timing vulnerabilities in payment/reward/credit systems to manipulate financial transactions",
                "technique": "Multiple simultaneous requests to exploit TOCTOU vulnerabilities in critical business operations",
                "category": "Business Logic",
                "difficulty": 7,
                "platform": "Web Applications",
                "tools_required": ["curl", "burp", "custom scripts"],
                "impact_level": "Critical",
                "payout_range": "5000-50000"
            },
            {
                "name": "JWT None Algorithm Attack",
                "description": "Bypassing authentication by manipulating JWT algorithm field to 'none' or causing key confusion",
                "technique": "Modify JWT header to use 'none' algorithm or exploit asymmetric/symmetric key confusion",
                "category": "Authentication Bypass",
                "difficulty": 5,
                "platform": "API/Web",
                "tools_required": ["jwt.io", "jwt_tool", "custom decoder"],
                "impact_level": "Critical",
                "payout_range": "10000-25000"
            },
            {
                "name": "AI/LLM Prompt Injection",
                "description": "Jailbreaking language models to bypass safety restrictions and perform unintended actions",
                "technique": "Crafted prompts to manipulate LLM reasoning and bypass content filters or access restrictions",
                "category": "AI/ML Security",
                "difficulty": 6,
                "platform": "AI Services",
                "tools_required": ["custom prompts", "system analysis"],
                "impact_level": "High",
                "payout_range": "10000-100000"
            },
            {
                "name": "BOLA/IDOR API Exploitation",
                "description": "Direct object reference vulnerabilities allowing unauthorized access to resources via predictable identifiers",
                "technique": "Sequential ID enumeration and parameter manipulation to access unauthorized data",
                "category": "Authorization",
                "difficulty": 4,
                "platform": "REST APIs",
                "tools_required": ["burp", "postman", "custom scripts"],
                "impact_level": "High",
                "payout_range": "500-5000"
            },
            {
                "name": "Chainlink ACE Policy Bypass",
                "description": "Exploiting inconsistencies between onchain and offchain policy enforcement in compliance engine",
                "technique": "Manipulate policy execution differences between hybrid verification systems",
                "category": "Smart Contract",
                "difficulty": 8,
                "platform": "Blockchain",
                "tools_required": ["hardhat", "foundry", "web3.js"],
                "impact_level": "Critical",
                "payout_range": "100000-500000"
            },
            {
                "name": "Cross-Chain TOCTOU Exploitation",
                "description": "Time-of-check vs time-of-use vulnerabilities in cross-chain message passing and verification",
                "technique": "Exploit timing delays between chain verification states and policy enforcement",
                "category": "Cross-Chain",
                "difficulty": 9,
                "platform": "Multi-Chain",
                "tools_required": ["chainlink tools", "cross-chain monitors"],
                "impact_level": "Critical",
                "payout_range": "50000-300000"
            }
        ]
        
        embeddings_data = []
        
        for pattern in attack_patterns:
            # Create comprehensive text for embedding
            pattern_text = f"""
Attack Pattern: {pattern['name']}
Category: {pattern['category']}
Description: {pattern['description']}
Technique: {pattern['technique']}
Platform: {pattern['platform']}
Difficulty: {pattern['difficulty']}/10
Impact: {pattern['impact_level']}
Tools: {', '.join(pattern['tools_required'])}
Payout Range: ${pattern['payout_range']}
"""
            
            # Generate embedding
            embedding = self.model.encode([pattern_text])[0].tolist()
            
            # Create payload
            payload = {
                "id": str(uuid.uuid4()),
                "name": pattern["name"],
                "description": pattern["description"],
                "technique": pattern["technique"],
                "category": pattern["category"],
                "difficulty": pattern["difficulty"],
                "platform": pattern["platform"],
                "tools_required": pattern["tools_required"],
                "impact_level": pattern["impact_level"],
                "payout_range": pattern["payout_range"],
                "source": "attack_patterns",
                "text_content": pattern_text.strip(),
                "created_at": datetime.now().isoformat()
            }
            
            embeddings_data.append({
                "id": payload["id"],
                "vector": embedding,
                "payload": payload
            })
        
        logger.info(f"Generated {len(embeddings_data)} attack pattern embeddings")
        return embeddings_data
    
    def generate_evidence_embeddings(self) -> List[Dict[str, Any]]:
        """Generate embeddings for evidence packages and PoC descriptions."""
        logger.info("Generating evidence embeddings...")
        
        evidence_patterns = [
            {
                "name": "Race Condition PoC Template",
                "description": "Multi-threaded proof of concept demonstrating timing vulnerabilities in payment systems",
                "code_type": "Python/Threading",
                "complexity": 6,
                "poc_structure": "Setup concurrent requests, exploit timing window, demonstrate unauthorized state change",
                "verification": "Monitor account balances, check for double-spend or credit manipulation",
                "success_indicators": ["Balance inconsistency", "Unauthorized credit", "Transaction log gaps"],
                "poc_type": "Concurrency",
                "language": "python",
                "verified": True,
                "success_rate": 0.75
            },
            {
                "name": "JWT Manipulation Exploit",
                "description": "Authentication bypass through JWT algorithm manipulation and key confusion attacks",
                "code_type": "Web/API",
                "complexity": 4,
                "poc_structure": "Decode JWT, modify algorithm to none, re-encode, submit to protected endpoint",
                "verification": "Successful access to protected resources without valid authentication",
                "success_indicators": ["Admin access", "Protected data retrieval", "Privilege escalation"],
                "poc_type": "Authentication",
                "language": "javascript",
                "verified": True,
                "success_rate": 0.85
            },
            {
                "name": "Smart Contract State Manipulation",
                "description": "Blockchain state manipulation through transaction ordering and frontrunning attacks",
                "code_type": "Solidity/Web3",
                "complexity": 8,
                "poc_structure": "Deploy test contract, execute state-changing transaction, demonstrate unauthorized modification",
                "verification": "Compare expected vs actual contract state, verify unauthorized changes",
                "success_indicators": ["State corruption", "Unauthorized balance changes", "Logic bypass"],
                "poc_type": "Blockchain",
                "language": "solidity",
                "verified": False,
                "success_rate": 0.60
            },
            {
                "name": "API Parameter Pollution",
                "description": "HTTP parameter pollution to bypass input validation and access controls",
                "code_type": "HTTP/REST",
                "complexity": 5,
                "poc_structure": "Craft polluted parameters, submit to vulnerable endpoint, demonstrate bypass",
                "verification": "Successful bypass of validation, unauthorized data access or modification",
                "success_indicators": ["Validation bypass", "Unauthorized access", "Data manipulation"],
                "poc_type": "Web API",
                "language": "curl",
                "verified": True,
                "success_rate": 0.70
            }
        ]
        
        embeddings_data = []
        
        for evidence in evidence_patterns:
            # Create comprehensive text for embedding
            evidence_text = f"""
Evidence Package: {evidence['name']}
Type: {evidence['poc_type']}
Description: {evidence['description']}
Language: {evidence['language']}
Complexity: {evidence['complexity']}/10
Structure: {evidence['poc_structure']}
Verification: {evidence['verification']}
Success Indicators: {', '.join(evidence['success_indicators'])}
Success Rate: {evidence['success_rate']*100}%
Code Type: {evidence['code_type']}
"""
            
            # Generate embedding
            embedding = self.model.encode([evidence_text])[0].tolist()
            
            # Create payload
            payload = {
                "id": str(uuid.uuid4()),
                "name": evidence["name"],
                "description": evidence["description"],
                "poc_type": evidence["poc_type"],
                "language": evidence["language"],
                "complexity": evidence["complexity"],
                "poc_structure": evidence["poc_structure"],
                "verification": evidence["verification"],
                "success_indicators": evidence["success_indicators"],
                "verified": evidence["verified"],
                "success_rate": evidence["success_rate"],
                "code_type": evidence["code_type"],
                "source": "evidence_patterns",
                "text_content": evidence_text.strip(),
                "created_at": datetime.now().isoformat()
            }
            
            embeddings_data.append({
                "id": payload["id"],
                "vector": embedding,
                "payload": payload
            })
        
        logger.info(f"Generated {len(embeddings_data)} evidence embeddings")
        return embeddings_data
    
    def generate_methodology_embeddings(self) -> List[Dict[str, Any]]:
        """Generate embeddings for research methodologies and frameworks."""
        logger.info("Generating methodology embeddings...")
        
        methodologies = [
            {
                "framework": "T.K.V.F.",
                "name": "Technology Knowledge Verification Framework",
                "description": "Systematic approach to verify production deployment status before vulnerability research",
                "phase": "Pre-Research",
                "steps": [
                    "Check if technology is actively used in production",
                    "Search for deprecation notices and archived repositories",
                    "Find real mainnet deployments and contract addresses",
                    "Verify protocol usage by major platforms",
                    "Check recent activity and transaction volume"
                ],
                "effectiveness": 0.95,
                "lessons_learned": "Prevents false positives by verifying production reality vs GitHub code",
                "prevent_failures": ["Deprecated code research", "Testnet-only vulnerabilities", "Legacy system bugs"]
            },
            {
                "framework": "D.I.E.",
                "name": "Demonstrable, Impactful, Evidentiary Framework",
                "description": "Quality standards for vulnerability validation before submission",
                "phase": "Pre-Submission",
                "steps": [
                    "Develop working proof-of-concept demonstrating the vulnerability",
                    "Quantify security impact and potential business consequences",
                    "Compile comprehensive evidence package with technical details"
                ],
                "effectiveness": 0.90,
                "lessons_learned": "Ensures high-quality submissions with working PoCs and clear impact",
                "prevent_failures": ["Theoretical vulnerabilities", "Low-impact findings", "Insufficient evidence"]
            },
            {
                "framework": "Hybrid Verification",
                "name": "Multi-Agent Competition Analysis",
                "description": "Deploy specialized agents to search for existing vulnerabilities across platforms",
                "phase": "Research Planning",
                "steps": [
                    "Deploy prior-art-researcher agent for comprehensive searches",
                    "Search HackerOne, Immunefi, CVE databases, GitHub advisories",
                    "Analyze competition and duplicate risk assessment",
                    "Verify novelty before investing research time"
                ],
                "effectiveness": 0.85,
                "lessons_learned": "49% of initial findings were duplicates - verification saves significant time",
                "prevent_failures": ["Duplicate submissions", "Already-patched vulnerabilities", "Competitive research areas"]
            },
            {
                "framework": "Sequential Thinking",
                "name": "Structured Logic Verification Process", 
                "description": "Step-by-step logical analysis to validate vulnerability assumptions",
                "phase": "Vulnerability Analysis",
                "steps": [
                    "Break down vulnerability into component assumptions",
                    "Verify each assumption against smart contract code",
                    "Test logical flow and identify potential failure points",
                    "Validate economic impact and exploitation feasibility"
                ],
                "effectiveness": 0.92,
                "lessons_learned": "Prevented Ondo T+2 false positive by identifying flawed assumption about token burning",
                "prevent_failures": ["Logic errors", "Assumption-based research", "Incomplete analysis"]
            }
        ]
        
        embeddings_data = []
        
        for methodology in methodologies:
            # Create comprehensive text for embedding
            method_text = f"""
Research Framework: {methodology['name']} ({methodology['framework']})
Phase: {methodology['phase']}
Description: {methodology['description']}
Steps: {'; '.join(methodology['steps'])}
Effectiveness: {methodology['effectiveness']*100}%
Lessons Learned: {methodology['lessons_learned']}
Prevents: {', '.join(methodology['prevent_failures'])}
"""
            
            # Generate embedding
            embedding = self.model.encode([method_text])[0].tolist()
            
            # Create payload
            payload = {
                "id": str(uuid.uuid4()),
                "framework": methodology["framework"],
                "name": methodology["name"],
                "description": methodology["description"],
                "phase": methodology["phase"],
                "steps": methodology["steps"],
                "effectiveness": methodology["effectiveness"],
                "lessons_learned": methodology["lessons_learned"],
                "prevent_failures": methodology["prevent_failures"],
                "source": "research_methodology",
                "text_content": method_text.strip(),
                "created_at": datetime.now().isoformat()
            }
            
            embeddings_data.append({
                "id": payload["id"],
                "vector": embedding,
                "payload": payload
            })
        
        logger.info(f"Generated {len(embeddings_data)} methodology embeddings")
        return embeddings_data
    
    def generate_platform_embeddings(self) -> List[Dict[str, Any]]:
        """Generate embeddings for target platforms and technology patterns."""
        logger.info("Generating platform embeddings...")
        
        platforms = [
            {
                "technology": "Chainlink ACE",
                "name": "Automated Compliance Engine",
                "description": "Institutional-grade compliance engine for $100T+ in traditional finance migration to blockchain",
                "version": "Production v1.0",
                "deployment_status": "Active Production",
                "launch_date": "June 2025",
                "vulnerability_patterns": [
                    "Hybrid execution policy bypass - onchain vs offchain inconsistencies",
                    "Multi-layered policy conflicts - jurisdiction, accreditation, sanctions rule interactions",
                    "Cross-chain TOCTOU - time-of-check vs time-of-use in verification",
                    "CCID identity spoofing - cryptographic proof manipulation"
                ],
                "audit_history": "Early stage, limited third-party audits due to recent launch",
                "risk_level": 9,
                "bounty_range": "200000-500000",
                "competition_level": "Low - newly launched"
            },
            {
                "technology": "Chainlink CRE",
                "name": "Compute Runtime Environment",
                "description": "Serverless compute platform for decentralized applications with WASM isolation",
                "version": "MVP Q1 2025",
                "deployment_status": "Early Stage Production",
                "launch_date": "Q1 2025",
                "vulnerability_patterns": [
                    "Module composition bugs - conflicting state assumptions between modules",
                    "Sandbox escape - WASM/container isolation bypass",
                    "Resource exhaustion - gas griefing and DoS attacks",
                    "Re-entrancy between module calls"
                ],
                "audit_history": "Minimal auditing, early deployment phase",
                "risk_level": 8,
                "bounty_range": "100000-300000",
                "competition_level": "Very Low - cutting edge technology"
            },
            {
                "technology": "Stellar Soroban",
                "name": "Smart Contracts Platform",
                "description": "Rust-based smart contract platform with unique storage and execution model",
                "version": "Production v20.0+",
                "deployment_status": "Active Production",
                "launch_date": "October 2024",
                "vulnerability_patterns": [
                    "Unbounded storage DoS - instance storage loads all data each call",
                    "Type conversion failures - Vec<T> to Vals conversion bugs",
                    "Cross-contract re-entrancy - new platform missing guards",
                    "Rust-specific memory safety issues"
                ],
                "audit_history": "Multiple audits but relatively new platform",
                "risk_level": 7,
                "bounty_range": "50000-250000",
                "competition_level": "Medium - established but evolving"
            },
            {
                "technology": "EIP-7702",
                "name": "Account Abstraction Standard",
                "description": "Ethereum improvement proposal enabling batch transactions and account abstraction",
                "version": "Ethereum Mainnet",
                "deployment_status": "Recently Deployed",
                "launch_date": "2025",
                "vulnerability_patterns": [
                    "Batch transaction phishing - hidden malicious transactions in batches",
                    "Transaction parameter manipulation - alter parameters post-signing",
                    "Bundler/Paymaster exploitation - AA wallet specific attacks",
                    "Intent-based architecture vulnerabilities"
                ],
                "audit_history": "Core protocol audited but implementation vulnerabilities possible",
                "risk_level": 6,
                "bounty_range": "50000-200000",
                "competition_level": "High - actively researched"
            },
            {
                "technology": "RWA Protocols",
                "name": "Real World Asset Tokenization",
                "description": "Protocols bridging traditional financial assets to blockchain infrastructure",
                "version": "Various implementations",
                "deployment_status": "Production across multiple protocols",
                "launch_date": "2024-2025",
                "vulnerability_patterns": [
                    "Oracle manipulation for real asset pricing",
                    "Legal/technical interface vulnerabilities",
                    "Settlement delay exploitations",
                    "Cross-system validation failures"
                ],
                "audit_history": "Varies by implementation, $14.6M stolen in H1 2025",
                "risk_level": 8,
                "bounty_range": "100000-2000000",
                "competition_level": "High - active exploitation"
            }
        ]
        
        embeddings_data = []
        
        for platform in platforms:
            # Create comprehensive text for embedding
            platform_text = f"""
Technology: {platform['technology']} - {platform['name']}
Status: {platform['deployment_status']}
Version: {platform['version']}
Launch: {platform['launch_date']}
Description: {platform['description']}
Vulnerability Patterns: {'; '.join(platform['vulnerability_patterns'])}
Risk Level: {platform['risk_level']}/10
Bounty Range: ${platform['bounty_range']}
Competition: {platform['competition_level']}
Audit History: {platform['audit_history']}
"""
            
            # Generate embedding
            embedding = self.model.encode([platform_text])[0].tolist()
            
            # Create payload
            payload = {
                "id": str(uuid.uuid4()),
                "technology": platform["technology"],
                "name": platform["name"],
                "description": platform["description"],
                "version": platform["version"],
                "deployment_status": platform["deployment_status"],
                "launch_date": platform["launch_date"],
                "vulnerability_patterns": platform["vulnerability_patterns"],
                "audit_history": platform["audit_history"],
                "risk_level": platform["risk_level"],
                "bounty_range": platform["bounty_range"],
                "competition_level": platform["competition_level"],
                "source": "target_platforms",
                "text_content": platform_text.strip(),
                "created_at": datetime.now().isoformat()
            }
            
            embeddings_data.append({
                "id": payload["id"],
                "vector": embedding,
                "payload": payload
            })
        
        logger.info(f"Generated {len(embeddings_data)} platform embeddings")
        return embeddings_data
    
    def upload_to_qdrant(self, collection_name: str, embeddings_data: List[Dict[str, Any]]) -> bool:
        """Upload embeddings data to a specific Qdrant collection."""
        logger.info(f"Uploading {len(embeddings_data)} embeddings to {collection_name}...")
        
        try:
            # Batch upload for efficiency
            batch_size = 100
            for i in range(0, len(embeddings_data), batch_size):
                batch = embeddings_data[i:i + batch_size]
                
                # Prepare points for Qdrant
                points = {
                    "points": [
                        {
                            "id": item["id"],
                            "vector": item["vector"],
                            "payload": item["payload"]
                        }
                        for item in batch
                    ]
                }
                
                # Upload batch
                response = requests.put(
                    f"{self.qdrant_url}/collections/{collection_name}/points",
                    json=points
                )
                
                if response.status_code not in [200, 201]:
                    logger.error(f"Failed to upload batch to {collection_name}: {response.text}")
                    return False
                
                logger.info(f"Uploaded batch {i//batch_size + 1}/{(len(embeddings_data) + batch_size - 1)//batch_size}")
            
            logger.info(f"‚úÖ Successfully uploaded all embeddings to {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"Error uploading to {collection_name}: {e}")
            return False
    
    def _create_vulnerability_text(self, vuln: sqlite3.Row) -> str:
        """Create comprehensive text representation of vulnerability for embedding."""
        text_parts = []
        
        # Basic info
        if vuln["type"]:
            text_parts.append(f"Vulnerability Type: {vuln['type']}")
        
        if vuln["severity"]:
            text_parts.append(f"Severity: {vuln['severity']}")
        
        if vuln["pattern"]:
            text_parts.append(f"Attack Pattern: {vuln['pattern']}")
        
        # Financial impact
        if vuln["payout_min"] and vuln["payout_max"]:
            text_parts.append(f"Payout Range: ${vuln['payout_min']}-${vuln['payout_max']}")
        
        # Technical details
        if vuln["exploit_code"]:
            text_parts.append(f"Exploit Technique: {vuln['exploit_code']}")
        
        if vuln["cvss_score"]:
            text_parts.append(f"CVSS Score: {vuln['cvss_score']}")
        
        if vuln["cve"]:
            text_parts.append(f"CVE: {vuln['cve']}")
        
        # Contextual info
        if vuln["effort_ratio"]:
            text_parts.append(f"Effort Level: {vuln['effort_ratio']}")
        
        if vuln["program_handle"]:
            text_parts.append(f"Target Program: {vuln['program_handle']}")
        
        return " | ".join(text_parts)
    
    def create_search_indexes(self) -> bool:
        """Create search indexes for efficient querying."""
        logger.info("Creating search indexes...")
        
        try:
            for collection_name, config in self.collections_config.items():
                # Create payload indexes for filtering
                for field_name, field_config in config["payload_fields"].items():
                    index_config = {
                        "field_name": field_name,
                        "field_schema": field_config
                    }
                    
                    response = requests.put(
                        f"{self.qdrant_url}/collections/{collection_name}/index",
                        json=index_config
                    )
                    
                    if response.status_code == 200:
                        logger.info(f"‚úÖ Created index for {collection_name}.{field_name}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Index creation failed for {collection_name}.{field_name}: {response.text}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating search indexes: {e}")
            return False
    
    def generate_all_embeddings(self) -> bool:
        """Generate and upload all embeddings to Qdrant collections."""
        logger.info("üöÄ Starting comprehensive embeddings generation...")
        
        try:
            # Initialize model
            self.initialize_model()
            
            # Create collections
            if not self.create_collections():
                logger.error("Failed to create collections")
                return False
            
            # Generate and upload embeddings for each collection
            collections_data = {
                "vulnerability_research": self.generate_vulnerability_embeddings(),
                "attack_patterns": self.generate_attack_pattern_embeddings(),
                "evidence_packages": self.generate_evidence_embeddings(),
                "research_methodology": self.generate_methodology_embeddings(),
                "target_platforms": self.generate_platform_embeddings()
            }
            
            # Upload to Qdrant
            for collection_name, embeddings_data in collections_data.items():
                if not self.upload_to_qdrant(collection_name, embeddings_data):
                    logger.error(f"Failed to upload {collection_name}")
                    return False
            
            # Create search indexes
            self.create_search_indexes()
            
            # Generate summary report
            self._generate_summary_report(collections_data)
            
            logger.info("üéâ Successfully generated and uploaded all embeddings!")
            return True
            
        except Exception as e:
            logger.error(f"Error in embeddings generation: {e}")
            return False
    
    def _generate_summary_report(self, collections_data: Dict[str, List[Dict]]) -> None:
        """Generate a summary report of the embeddings generation process."""
        report = {
            "generation_timestamp": datetime.now().isoformat(),
            "total_embeddings": sum(len(data) for data in collections_data.values()),
            "collections": {
                name: {
                    "count": len(data),
                    "description": self.collections_config[name]["description"]
                }
                for name, data in collections_data.items()
            },
            "model_used": "all-MiniLM-L6-v2",
            "vector_dimension": 384,
            "status": "complete"
        }
        
        report_path = "/home/kali/bbhk/embeddings_generation_report.json"
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìä Summary report saved to: {report_path}")
        
        # Print summary
        print("\n" + "="*60)
        print("üéØ VECTOR EMBEDDINGS GENERATION COMPLETE")
        print("="*60)
        for collection_name, info in report["collections"].items():
            print(f"‚úÖ {collection_name}: {info['count']} embeddings")
        print(f"\nüìä Total Embeddings Generated: {report['total_embeddings']}")
        print(f"üß† Model: {report['model_used']} ({report['vector_dimension']}D)")
        print("="*60)

def main():
    """Main execution function."""
    logger.info("üéØ Vector Embeddings Generator for Vulnerability Research")
    logger.info("=" * 60)
    
    # Initialize generator
    generator = VulnerabilityEmbeddingsGenerator()
    
    # Generate all embeddings
    success = generator.generate_all_embeddings()
    
    if success:
        logger.info("üéâ Embeddings generation completed successfully!")
        logger.info("üîç Collections ready for semantic search")
        return 0
    else:
        logger.error("‚ùå Embeddings generation failed!")
        return 1

if __name__ == "__main__":
    exit(main())